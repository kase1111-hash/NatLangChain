module: llm_providers
version: "1.0"
description: "Multi-provider LLM abstraction â€” Anthropic, OpenAI, Gemini, Grok, Ollama, llama.cpp"
capabilities:
  network:
    - endpoint: "api.anthropic.com"
      protocol: "https"
      purpose: "Anthropic Claude API"
    - endpoint: "api.openai.com"
      protocol: "https"
      purpose: "OpenAI API"
    - endpoint: "generativelanguage.googleapis.com"
      protocol: "https"
      purpose: "Google Gemini API"
    - endpoint: "api.x.ai"
      protocol: "https"
      purpose: "xAI Grok API"
    - endpoint: "$OLLAMA_HOST"
      protocol: "http"
      purpose: "Local Ollama server"
    - endpoint: "$LLAMACPP_URL"
      protocol: "http"
      purpose: "Local llama.cpp server"
  filesystem:
    - path: "$ANTHROPIC_API_KEY"
      access: "read"
      purpose: "Provider API keys from environment"
    - path: "$OLLAMA_HOST"
      access: "read"
      purpose: "Local provider URLs from environment"
  shell:
    - command: "/usr/bin/llama-cli"
      purpose: "llama.cpp CLI for local LLM inference"
    - command: "/usr/local/bin/llama-cli"
      purpose: "llama.cpp CLI (alternate install path)"
    - command: "/opt/llama.cpp/llama-cli"
      purpose: "llama.cpp CLI (opt install path)"
  packages:
    - name: "anthropic"
      purpose: "Anthropic Claude API client"
    - name: "openai"
      purpose: "OpenAI API client"
    - name: "google.generativeai"
      purpose: "Google Gemini API client"
    - name: "httpx"
      purpose: "HTTP timeout configuration"
    - name: "requests"
      purpose: "HTTP calls to local providers"
